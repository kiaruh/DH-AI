{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar Bert usando Keras y tensorflow 2.0.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que definir la capa de MultiHeadAttention. Todavía no fue incluída como capa propiamente dicha en Keras, pero hay una implementación oficial en la página de keras que vamos a explicar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        ## que es este super?\n",
    "        ## super() te permite acceeder a los metodos de la super clase de la cual\n",
    "        ## la subclase está heredando. En este caso, estas herendando de Layers.\n",
    "        \n",
    "        ## definimos algunos parametros: cuantas cabezas va a tener self attention \n",
    "        ## y la dimensionalidad del embedding\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        ## la dimensionalidad del embedding tiene que ser divisible por el numero de cabezas.\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        \n",
    "        ## cuantas dimensiones va a tener cada cabeza:\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        \n",
    "        ## y definimos las capas de key, query y value.\n",
    "        ## son simplemente capas lineales\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        \n",
    "        ## y definimos la capa con la que combinamos las cabezas\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        \n",
    "        ## vamos a hacer lo que vimos en las diapos: \n",
    "        ## primero el producto entre el query y la transpuesta de key\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        \n",
    "        ## tomamos la cantidad de columnas de key y la usamos para escalar el score\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        \n",
    "        ## aplicamos la softmax para tener los attention weights y multiplicamos con values\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        ## vamos a armar la division en cabezas. \n",
    "        ## se va a entender mejor en el siguiente bloque de codigo\n",
    "        ## por ahora es solamente la forma en la que \n",
    "        ## reacomodamos los datos para armar las cabezas\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        ## vamos a ir dejando anotado la forma de cada tensor para que sea mas facil de seguir\n",
    "        \n",
    "        ## cuando empezamos:\n",
    "        ## x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        ## es decir, tenemos batch_size casos, con seq_len cantidad de vectores\n",
    "        ## (1 por cada token), y cada vector tiene embedding_dim dimensiones\n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        ## al costado de cada linea vamos a marcar el tamaño que queda el tensor:\n",
    "        ## para query, key y value vamos a tener tensores con batch_size casos, \n",
    "        ## con un largo de seq_len y la ultima dimension pasa a ser embed_dim, \n",
    "        ## que es el tamaño que definimos arriba en las Dense layers.\n",
    "        ## Es decir, solo modificamos la 3er dimension del tensor.\n",
    "        ## va a ser igual para query, key y value\n",
    "        query = self.query_dense(inputs)  ## (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        ## ahora vamos a separar en las cabezas:\n",
    "        query = self.separate_heads(query, batch_size)  \n",
    "        ## nos va a quedar:\n",
    "        ## (batch_size, num_heads, seq_len, projection_dim)\n",
    "        \n",
    "        ## repetimos:\n",
    "        key = self.separate_heads(key, batch_size)  \n",
    "        value = self.separate_heads(value, batch_size)  \n",
    "        \n",
    "                \n",
    "        ## ahora vamos a usar la capa atencional y vamos a obtener la salida y los pesos\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        \n",
    "        ## y reorganizamos la salida de tal manera que me quede:\n",
    "        ## (batch_size, seq_len, num_heads, projection_dim)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  \n",
    "        \n",
    "        \n",
    "        ## y ahora concatenamos las cabezas!\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  \n",
    "        ## el tamaño ahora es (batch_size, seq_len, embed_dim)\n",
    "        ## que s el mismo que teníamos al principio en query, key y value\n",
    "        \n",
    "        ## y metemos la ultima capa densa:\n",
    "        output = self.combine_heads(concat_attention)  \n",
    "        ## nos queda: (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya definimos la capa de multihead attention, pasamos a definir un bloque de transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        ## definimos una capa de multihead attention\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        \n",
    "        ## definimos una secuencia de capa lineal, relu (en el paper original usan gelu)\n",
    "        ## y capa lineal. Como el input van a ser batches de 2d, esto se va a aplicar\n",
    "        ## como una time distributed layer, es decir, se va a aplicar a cada timestep\n",
    "        \n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        \n",
    "        ## definimos capas de layernormalization y de dropout\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        ## aplicamos atencion y dropout\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        ## hacemos una residual connection y layer normalization\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ## tenemos la position-wise feed forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        \n",
    "        ## y repetimos dropout, residual connection y layer normalization\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya casi estamos. Solamente nos hace falta la capa de embeddings.  \n",
    "Necesitamos el embedding del token y el embedding de la posición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emded_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        \n",
    "        ## instanciamos dos capas de Embedding\n",
    "        ## una de vocabulario x dimension del embedding\n",
    "        ## otra de largo de secuencia x dimension del embedding\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        ## generamos un tensor de posicion para cada token\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        \n",
    "        ## usamos la posicion para el embedding\n",
    "        positions = self.pos_emb(positions)\n",
    "        \n",
    "        ## tomamos el embedding de cada token\n",
    "        x = self.token_emb(x)\n",
    "        \n",
    "        ## y los sumamos\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos los ingredientos básicos para armar un transformer. Vamos a unir todo. Vamos a hacer algo que podamos entrenar en clase, así que en vez de hacer el tamaño total, vamos a hacer uno \"mini\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Vamos a considerar solamente 20k palabras\n",
    "maxlen = 200  #  Solo considerar las primeras 200 palabras\n",
    "\n",
    "embed_dim = 32  # Tamaño del embedding\n",
    "num_heads = 2  # Numero de attention heads\n",
    "ff_dim = 32  # Tamaño de la hidden layer adentro del transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "x = transformer_block(x)\n",
    "\n",
    "## la salida del bloque la vamos a poolear con un global average pooling:\n",
    "## si tenemos (batch, seq, embed), hacemos el promedio y nos queda (batch, embed)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si! podemos usarlo para lo que queramos. Por ahora vayamos con algo sencillo: clasificacion de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "## me traigo el dataset de imdb (nota: acá no está con sentencepiece)\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## y voy a entrenarlo con solo 1 epoch a ver como da!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "25000/25000 [==============================] - 103s 4ms/sample - loss: 0.3756 - accuracy: 0.8268 - val_loss: 0.2906 - val_accuracy: 0.8766\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=1, validation_data=(x_val, y_val)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
