{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AXOxwebVb15"
   },
   "source": [
    "# Inteligencia Artificial\n",
    "# Clase 14 - Deep Learning para Computer Vision 2\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "En la práctica anterior, vimos cómo la *data augmentation* nos permitió combatir el sobreajuste de una red neuronal convolucional entrenada sobre un dataset relativamente pequeño, de apenas 2000 muestras (1000 por cada clase). Gracias a esta técnica de regularización, pudimos mejorar en 10 puntos porcentuales el accuracy de nuestro clasificador de imágenes de perros y gatos.\n",
    "\n",
    "En esta notebook, presentaremos otro enfoque común y altamente efectivo en *deep learning* que consiste en usar una red preentrenada. Esta técnica, conocida como *transfer learning*, nos permite aprovechar un modelo que ya ha sido entrenado previamente en un gran conjunto de datos (por ejemplo, el dataset de [ImageNet](http://www.image-net.org/)) y reutilizarlo en el contexto de un problema específico. Si el conjunto de datos con el que se entrenó el modelo es lo suficientemente grande y general, entonces la jerarquía espacial de *features* aprendida por la red puede actuar efectivamente como un modelo genérico del mundo visual y, por lo tanto, las *features* pueden resultar útiles para muchos problemas de *computer vision* diferentes, a pesar de que éstos sean completamente diferentes a la tarea original.\n",
    "\n",
    "Veremos que Keras ofrece en el módulo de aplicaciones distintos modelos destacados, previamente entrenadas sobre ImageNet, para que podamos reutilizarlos y aplicarlos a nuestros propios datasets. En este caso, trabajaremos con el modelo [VGG16](https://arxiv.org/abs/1409.1556) para mejorar la *performance* de nuestro clasificador de perros y gatos.\n",
    "\n",
    "<img src=\"https://distilledai.com/wp-content/uploads/2020/04/cat-vs-dog.jpeg\" width=500 />\n",
    "\n",
    "El contenido de esta notebook se basa mayormente en un ejemplo del [capítulo 5 del libro Deep Learning with Python](https://livebook.manning.com/book/deep-learning-with-python/chapter-5/), de François Chollet (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "! echo '{\"username\":\"mggaska\",\"key\":\"10bb9c3aaa775f3385c5c7e3a17a3eaf\"}' >> kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"username\":\"mggaska\",\"key\":\"10bb9c3aaa775f3385c5c7e3a17a3eaf\"}\r\n"
     ]
    }
   ],
   "source": [
    "cat ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘kaggle_original_data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir kaggle_original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.5.6)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kaggle) (4.46.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.6/site-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kaggle) (2.7.3)\n",
      "Requirement already satisfied: python-slugify in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kaggle) (1.23)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from kaggle) (1.11.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->kaggle) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from python-slugify->kaggle) (1.3)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "TOHm1V9YJ_kt",
    "outputId": "3aaf6189-8fff-4260-fd01-ba559807e107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs-vs-cats.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  dogs-vs-cats.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [unzip tes]\n",
      "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [t1.zip]\n",
      "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [unzip tra]\n",
      "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [in.zip]\n",
      "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd kaggle_original_data\n",
    "kaggle competitions download -c dogs-vs-cats\n",
    "unzip dogs-vs-cats.zip\n",
    "unzip test1.zip\n",
    "unzip train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8km1hQEKEzV"
   },
   "outputs": [],
   "source": [
    "# Estructuramos los directorios de trabajo\n",
    "import os, shutil\n",
    "\n",
    "# El path al directorio donde se descomprimió el dataset\n",
    "original_dataset_dir = './kaggle_original_data/train'\n",
    "\n",
    "# El directorio donde guardaremos el más pequeño\n",
    "base_dir = './cats_and_dogs_small'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Directorios para los splits de\n",
    "# entrenamiento, validación y test\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Directorio con imágenes de entrenamiento de gatos\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.makedirs(train_cats_dir, exist_ok=True)\n",
    "\n",
    "# Directorio con imágenes de entrenamiento de perros\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.makedirs(train_dogs_dir, exist_ok=True)\n",
    "\n",
    "# Directorio con las imágenes de validación de gatos\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.makedirs(validation_cats_dir, exist_ok=True)\n",
    "\n",
    "# Directorio con las imágenes de validación de perros\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.makedirs(validation_dogs_dir, exist_ok=True)\n",
    "\n",
    "# Directorio con imágenes de test de gatos\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.makedirs(test_cats_dir, exist_ok=True)\n",
    "\n",
    "# Directorio con las imágenes de test de perros\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.makedirs(test_dogs_dir, exist_ok=True)\n",
    "\n",
    "# Copiamos las primeras 1000 imágenes de gatos a train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Las siguientes 500 a validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copiamos las siguientes 500 a test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    " # Copiamos las primeras 1000 imágenes de perros a train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Las siguientes 500 a validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Y las siguientes 500 a test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XhK1PjoAVb16"
   },
   "source": [
    "## VGG16\n",
    "\n",
    "Vamos a utilizar la arquitectura VGG16, desarrollada por Karen Simonyan y Andrew\n",
    "Zisserman en el 2014. Este modelo es una *convnet* relativamente simple en su estructura, no muy diferente a las redes convolucionales con las que trabajamos hasta ahora, aunque sí mucho más profunda.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Jose_Cano31/publication/327070011/figure/fig1/AS:660549306159105@1534498635256/VGG-16-neural-network-architecture.png\"/>\n",
    "\n",
    "Podemos importar la clase `VGG16` del módulo [`applications`](https://keras.io/applications/) de Keras y generar una instancia del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "-JItwqm8Vb18",
    "outputId": "d8019c95-c56c-4339-cacc-4f578ce2845b"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwRT2Kqq__y2"
   },
   "source": [
    "Especificamos tres argumentos en el constructor:\n",
    "- `weights` define los pesos con los que inicializar el modelo. Puede ser `imagenet`, en caso de que deseemos utilizar el modelo preentrenado en este dataset, o `None`, si deseamos trabajar únicamente con la arquitectura de la red pero inicializar los pesos de manera aleatoria.\n",
    "- `include_top` se refiere a la inclusión (o no) de las capas densamente conectadas que se encuentran al final de la red original y que permiten clasificar las 1000 clases de ImageNet. Como debemos resolver una clasificación binaria, no necesitaremos incluir las capas densas en nuestro caso. Por lo tanto, sólo trabajaremos con la base convolucional del modelo VGG16. \n",
    "- `input_shape` corresponde a la forma del tensor de imágenes con el que alimentaremos la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKe_z1gKVb1-"
   },
   "source": [
    "Veamos la base convolucional de VGG16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "colab_type": "code",
    "id": "Pp8sjcNMVb1-",
    "outputId": "078b4692-04aa-4e47-d69d-6d9318847c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3vbcjnFDZVp"
   },
   "source": [
    "Hay dos maneras en que podemos usar una red preentrenada: *feature extraction* y *fine-tuning*. Vamos a cubrir ambas, comenzando por *feature extraction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCWJEzN_DpHG"
   },
   "source": [
    "## Feature extraction\n",
    "\n",
    "***Feature extraction* consiste en usar las representaciones aprendidas por una red preentrenada para extraer atributos interesantes de nuevas muestras. Estas *features* son luego utilizadas por un nuevo clasificador, que entrenamos desde cero.**\n",
    "\n",
    "Notemos que el mapa de features final de la VGG16 tiene forma `(4, 4, 512)`. Ésta será la entrada de un clasificador densamente conectado.\n",
    "\n",
    "En este punto tenemos dos maneras para proceder:\n",
    "\n",
    "* Correr la base convolucional sobre nuestro dataset, guardar su salida en una matriz Numpy en el disco, y luego usar esta información como entrada para un clasificador densamente conectado e independiente similar a los que ya vimos. \n",
    "Esta solución es rápida y barata de ejecutar, porque sólo requiere ejecutar la base convolucional una vez para cada imagen de entrada, y la base convolucional es, por mucho, la parte más costosa del *pipeline*. Pero por la misma razón, esta técnica no permite hacer *data augmentation*.\n",
    "\n",
    "* Extender el modelo que tenemos agregando capas densas en la parte superior, y entrenar la red de punta a punta sobre los datos de entrada. Esto nos permitirá usar *data augmentation*, porque cada imagen de entrada pasa por la base convolucional cada vez que es vista por el modelo. Pero por la misma razón, esta técnica es mucho más costosa que la primero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gx0QNMYXg_BE"
   },
   "source": [
    "### Feature extraction rápida sin data augmentation\n",
    "\n",
    "Primero veremos la técnica de *feature extraction* rápida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "R5PCL-RoVb2D",
    "outputId": "a2cbf198-a093-44e1-c329-f5f46647727b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = './cats_and_dogs_small'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "  \n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    \n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Notar que este generador devuelve data indefinidamente en un loop,\n",
    "            # debemos cortarlo con un \"break\" despues de que cada imagen haya sido vista una vez\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b34mxtJtVb2F"
   },
   "source": [
    "Las *features* extraídas son de forma `(samples, 4, 4, 512)`. Con ellas vamos a alimentar un clasificador densamente conectado, así que primero tenemos que aplanarlas a la forma `(samples, 8192)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSquTbSIVb2F"
   },
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_QeM2XaVb2H"
   },
   "source": [
    "Ahora definimos el clasificador y entrenamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DbXMrbuzVb2H",
    "outputId": "b8140807-49ec-4d7e-8347-29e741d6fba0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 663us/sample - loss: 0.6117 - acc: 0.6635 - val_loss: 0.4591 - val_acc: 0.8340\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 376us/sample - loss: 0.4335 - acc: 0.8055 - val_loss: 0.3719 - val_acc: 0.8650\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 374us/sample - loss: 0.3583 - acc: 0.8560 - val_loss: 0.3453 - val_acc: 0.8530\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 377us/sample - loss: 0.3230 - acc: 0.8715 - val_loss: 0.3064 - val_acc: 0.8810\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 373us/sample - loss: 0.2900 - acc: 0.8810 - val_loss: 0.2867 - val_acc: 0.8920\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 379us/sample - loss: 0.2690 - acc: 0.8990 - val_loss: 0.2766 - val_acc: 0.8890\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 0.2468 - acc: 0.8975 - val_loss: 0.2651 - val_acc: 0.8960\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 394us/sample - loss: 0.2336 - acc: 0.9060 - val_loss: 0.2595 - val_acc: 0.8960\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 404us/sample - loss: 0.2196 - acc: 0.9150 - val_loss: 0.2550 - val_acc: 0.8920\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 407us/sample - loss: 0.2109 - acc: 0.9210 - val_loss: 0.2494 - val_acc: 0.9030\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 405us/sample - loss: 0.1976 - acc: 0.9260 - val_loss: 0.2466 - val_acc: 0.9060\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 418us/sample - loss: 0.1915 - acc: 0.9315 - val_loss: 0.2439 - val_acc: 0.9030\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 373us/sample - loss: 0.1876 - acc: 0.9315 - val_loss: 0.2422 - val_acc: 0.8990\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 371us/sample - loss: 0.1719 - acc: 0.9430 - val_loss: 0.2408 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 370us/sample - loss: 0.1658 - acc: 0.9435 - val_loss: 0.2369 - val_acc: 0.9040\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 370us/sample - loss: 0.1619 - acc: 0.9490 - val_loss: 0.2359 - val_acc: 0.9040\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 371us/sample - loss: 0.1568 - acc: 0.9395 - val_loss: 0.2367 - val_acc: 0.9020\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 373us/sample - loss: 0.1479 - acc: 0.9485 - val_loss: 0.2391 - val_acc: 0.9020\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 374us/sample - loss: 0.1373 - acc: 0.9550 - val_loss: 0.2463 - val_acc: 0.8950\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 0.1350 - acc: 0.9565 - val_loss: 0.2354 - val_acc: 0.9060\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 372us/sample - loss: 0.1269 - acc: 0.9585 - val_loss: 0.2365 - val_acc: 0.9030\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 373us/sample - loss: 0.1205 - acc: 0.9605 - val_loss: 0.2340 - val_acc: 0.9030\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 371us/sample - loss: 0.1191 - acc: 0.9585 - val_loss: 0.2371 - val_acc: 0.9020\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 0.1195 - acc: 0.9615 - val_loss: 0.2333 - val_acc: 0.9010\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 369us/sample - loss: 0.1119 - acc: 0.9650 - val_loss: 0.2373 - val_acc: 0.8990\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 371us/sample - loss: 0.1061 - acc: 0.9620 - val_loss: 0.2343 - val_acc: 0.9010\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 371us/sample - loss: 0.1040 - acc: 0.9675 - val_loss: 0.2351 - val_acc: 0.9010\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 374us/sample - loss: 0.0993 - acc: 0.9695 - val_loss: 0.2402 - val_acc: 0.8960\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 0.0912 - acc: 0.9710 - val_loss: 0.2462 - val_acc: 0.8970\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 377us/sample - loss: 0.0931 - acc: 0.9705 - val_loss: 0.2410 - val_acc: 0.8970\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 374us/sample - loss: 0.0884 - acc: 0.9730 - val_loss: 0.2379 - val_acc: 0.9000\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 376us/sample - loss: 0.0848 - acc: 0.9730 - val_loss: 0.2407 - val_acc: 0.8990\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 0.0823 - acc: 0.9755 - val_loss: 0.2421 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 374us/sample - loss: 0.0794 - acc: 0.9815 - val_loss: 0.2417 - val_acc: 0.9020\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 373us/sample - loss: 0.0735 - acc: 0.9790 - val_loss: 0.2456 - val_acc: 0.8990\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 376us/sample - loss: 0.0734 - acc: 0.9765 - val_loss: 0.2471 - val_acc: 0.8970\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 377us/sample - loss: 0.0704 - acc: 0.9835 - val_loss: 0.2456 - val_acc: 0.8990\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 0.0675 - acc: 0.9800 - val_loss: 0.2550 - val_acc: 0.8950\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 0.0647 - acc: 0.9845 - val_loss: 0.2554 - val_acc: 0.8970\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 0.0595 - acc: 0.9840 - val_loss: 0.2478 - val_acc: 0.9010\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 0.0600 - acc: 0.9865 - val_loss: 0.2544 - val_acc: 0.8970\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 0.0581 - acc: 0.9835 - val_loss: 0.2546 - val_acc: 0.8960\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 0.0552 - acc: 0.9835 - val_loss: 0.2549 - val_acc: 0.8980\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 377us/sample - loss: 0.0540 - acc: 0.9875 - val_loss: 0.2599 - val_acc: 0.8970\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 379us/sample - loss: 0.0482 - acc: 0.9925 - val_loss: 0.2533 - val_acc: 0.8990\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 0.0467 - acc: 0.9910 - val_loss: 0.2555 - val_acc: 0.9010\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 379us/sample - loss: 0.0506 - acc: 0.9875 - val_loss: 0.2617 - val_acc: 0.8960\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0452 - acc: 0.9945 - val_loss: 0.2732 - val_acc: 0.8980\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 0.0440 - acc: 0.9895 - val_loss: 0.2669 - val_acc: 0.8990\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 0.0432 - acc: 0.9920 - val_loss: 0.2848 - val_acc: 0.8910\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 0.0400 - acc: 0.9935 - val_loss: 0.2771 - val_acc: 0.8990\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 0.0373 - acc: 0.9930 - val_loss: 0.2661 - val_acc: 0.8960\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 0.0356 - acc: 0.9935 - val_loss: 0.2716 - val_acc: 0.8990\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 375us/sample - loss: 0.0349 - acc: 0.9950 - val_loss: 0.2830 - val_acc: 0.8990\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 379us/sample - loss: 0.0335 - acc: 0.9955 - val_loss: 0.2686 - val_acc: 0.9000\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 382us/sample - loss: 0.0311 - acc: 0.9955 - val_loss: 0.2726 - val_acc: 0.8970\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 0.0302 - acc: 0.9960 - val_loss: 0.2897 - val_acc: 0.8970\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 0.0290 - acc: 0.9970 - val_loss: 0.2737 - val_acc: 0.8990\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 387us/sample - loss: 0.0276 - acc: 0.9960 - val_loss: 0.2775 - val_acc: 0.8980\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 0.0267 - acc: 0.9980 - val_loss: 0.2800 - val_acc: 0.8970\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 0.0281 - acc: 0.9960 - val_loss: 0.2789 - val_acc: 0.8990\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 382us/sample - loss: 0.0253 - acc: 0.9980 - val_loss: 0.2843 - val_acc: 0.8980\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 0.0217 - acc: 0.9990 - val_loss: 0.2835 - val_acc: 0.8990\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0239 - acc: 0.9970 - val_loss: 0.2881 - val_acc: 0.8970\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 0.0220 - acc: 0.9975 - val_loss: 0.2887 - val_acc: 0.8960\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 0.0199 - acc: 0.9990 - val_loss: 0.2973 - val_acc: 0.8980\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0200 - acc: 0.9990 - val_loss: 0.2989 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 0.0193 - acc: 0.9985 - val_loss: 0.2985 - val_acc: 0.8970\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 0.0182 - acc: 0.9990 - val_loss: 0.3009 - val_acc: 0.8990\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 0.0160 - acc: 0.9995 - val_loss: 0.3150 - val_acc: 0.8940\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 0.0175 - acc: 0.9990 - val_loss: 0.2973 - val_acc: 0.8990\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 393us/sample - loss: 0.0152 - acc: 0.9990 - val_loss: 0.3237 - val_acc: 0.8960\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0143 - acc: 0.9990 - val_loss: 0.3131 - val_acc: 0.8990\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0136 - acc: 0.9995 - val_loss: 0.3100 - val_acc: 0.9020\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0138 - acc: 0.9985 - val_loss: 0.3081 - val_acc: 0.9020\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 0.0129 - acc: 0.9995 - val_loss: 0.3221 - val_acc: 0.8970\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 395us/sample - loss: 0.0126 - acc: 0.9985 - val_loss: 0.3119 - val_acc: 0.9030\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 0.0117 - acc: 1.0000 - val_loss: 0.3227 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0106 - acc: 0.9995 - val_loss: 0.3191 - val_acc: 0.9020\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0110 - acc: 0.9995 - val_loss: 0.3242 - val_acc: 0.8990\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 0.0107 - acc: 1.0000 - val_loss: 0.3305 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0102 - acc: 0.9995 - val_loss: 0.3269 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 0.0093 - acc: 0.9995 - val_loss: 0.3276 - val_acc: 0.9010\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 0.0089 - acc: 1.0000 - val_loss: 0.3329 - val_acc: 0.9020\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 0.0088 - acc: 1.0000 - val_loss: 0.3347 - val_acc: 0.9030\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 0.0074 - acc: 1.0000 - val_loss: 0.3496 - val_acc: 0.8970\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 398us/sample - loss: 0.0085 - acc: 0.9995 - val_loss: 0.3365 - val_acc: 0.8980\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 393us/sample - loss: 0.0069 - acc: 1.0000 - val_loss: 0.3470 - val_acc: 0.9010\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 0.0078 - acc: 1.0000 - val_loss: 0.3428 - val_acc: 0.8990\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.3739 - val_acc: 0.8900\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 394us/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.3457 - val_acc: 0.9010\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.3500 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.3557 - val_acc: 0.9040\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 0.0052 - acc: 1.0000 - val_loss: 0.3594 - val_acc: 0.9010\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 382us/sample - loss: 0.0047 - acc: 1.0000 - val_loss: 0.3680 - val_acc: 0.9030\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0060 - acc: 0.9995 - val_loss: 0.3679 - val_acc: 0.9030\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 382us/sample - loss: 0.0049 - acc: 1.0000 - val_loss: 0.3703 - val_acc: 0.9040\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 0.3830 - val_acc: 0.8960\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 0.0044 - acc: 1.0000 - val_loss: 0.3714 - val_acc: 0.9040\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 382us/sample - loss: 0.0040 - acc: 1.0000 - val_loss: 0.3778 - val_acc: 0.9030\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.3732 - val_acc: 0.9010\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3751 - val_acc: 0.9030\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.3890 - val_acc: 0.8950\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3912 - val_acc: 0.9010\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3948 - val_acc: 0.9010\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3899 - val_acc: 0.9020\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 0.4046 - val_acc: 0.8990\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.4106 - val_acc: 0.8930\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.4127 - val_acc: 0.8930\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4248 - val_acc: 0.8910\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.4148 - val_acc: 0.8990\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 0.4027 - val_acc: 0.9020\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 396us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.4153 - val_acc: 0.9000\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 397us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4159 - val_acc: 0.8980\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.4294 - val_acc: 0.8970\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.4244 - val_acc: 0.8980\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 392us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.4177 - val_acc: 0.9030\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.4297 - val_acc: 0.8970\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4394 - val_acc: 0.8980\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4578 - val_acc: 0.8890\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.4370 - val_acc: 0.8990\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4496 - val_acc: 0.8960\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4423 - val_acc: 0.9010\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4518 - val_acc: 0.8970\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4632 - val_acc: 0.8920\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4692 - val_acc: 0.8920\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 9.1652e-04 - acc: 1.0000 - val_loss: 0.4635 - val_acc: 0.8950\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4756 - val_acc: 0.8930\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 394us/sample - loss: 8.6088e-04 - acc: 1.0000 - val_loss: 0.4788 - val_acc: 0.8950\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4769 - val_acc: 0.8980\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 9.7572e-04 - acc: 1.0000 - val_loss: 0.4674 - val_acc: 0.8990\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 8.3262e-04 - acc: 1.0000 - val_loss: 0.4809 - val_acc: 0.8970\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 7.4202e-04 - acc: 1.0000 - val_loss: 0.4951 - val_acc: 0.8940\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 394us/sample - loss: 8.6923e-04 - acc: 1.0000 - val_loss: 0.4907 - val_acc: 0.8960\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 395us/sample - loss: 8.0605e-04 - acc: 1.0000 - val_loss: 0.4950 - val_acc: 0.8970\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 395us/sample - loss: 6.8805e-04 - acc: 1.0000 - val_loss: 0.4867 - val_acc: 0.8970\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 6.2898e-04 - acc: 1.0000 - val_loss: 0.5036 - val_acc: 0.8940\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 5.9437e-04 - acc: 1.0000 - val_loss: 0.5022 - val_acc: 0.8940\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 394us/sample - loss: 5.2553e-04 - acc: 1.0000 - val_loss: 0.4947 - val_acc: 0.8950\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 4.2976e-04 - acc: 1.0000 - val_loss: 0.5070 - val_acc: 0.8970\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 6.0667e-04 - acc: 1.0000 - val_loss: 0.5009 - val_acc: 0.8960\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 395us/sample - loss: 4.3364e-04 - acc: 1.0000 - val_loss: 0.5050 - val_acc: 0.8990\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 5.0509e-04 - acc: 1.0000 - val_loss: 0.5242 - val_acc: 0.8920\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 4.5114e-04 - acc: 1.0000 - val_loss: 0.5220 - val_acc: 0.8940\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 4.1546e-04 - acc: 1.0000 - val_loss: 0.5158 - val_acc: 0.8970\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 3.4073e-04 - acc: 1.0000 - val_loss: 0.5297 - val_acc: 0.8960\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 3.1845e-04 - acc: 1.0000 - val_loss: 0.5202 - val_acc: 0.8970\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 3.9859e-04 - acc: 1.0000 - val_loss: 0.5256 - val_acc: 0.8950\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 3.8758e-04 - acc: 1.0000 - val_loss: 0.5268 - val_acc: 0.8960\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 2.7177e-04 - acc: 1.0000 - val_loss: 0.5286 - val_acc: 0.8970\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 397us/sample - loss: 3.1603e-04 - acc: 1.0000 - val_loss: 0.5785 - val_acc: 0.8900\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 3.5909e-04 - acc: 1.0000 - val_loss: 0.5447 - val_acc: 0.8960\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 3.3025e-04 - acc: 1.0000 - val_loss: 0.5647 - val_acc: 0.8960\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 2.7872e-04 - acc: 1.0000 - val_loss: 0.5397 - val_acc: 0.8970\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 394us/sample - loss: 2.3512e-04 - acc: 1.0000 - val_loss: 0.5383 - val_acc: 0.9020\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 2.6831e-04 - acc: 1.0000 - val_loss: 0.5529 - val_acc: 0.8960\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 1.9098e-04 - acc: 1.0000 - val_loss: 0.5475 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 1.9197e-04 - acc: 1.0000 - val_loss: 0.5801 - val_acc: 0.8940\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 2.7309e-04 - acc: 1.0000 - val_loss: 0.5817 - val_acc: 0.8950\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 2.2894e-04 - acc: 1.0000 - val_loss: 0.5593 - val_acc: 0.8970\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 2.1870e-04 - acc: 1.0000 - val_loss: 0.5764 - val_acc: 0.8950\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 1.6324e-04 - acc: 1.0000 - val_loss: 0.5718 - val_acc: 0.8940\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 1.3128e-04 - acc: 1.0000 - val_loss: 0.5841 - val_acc: 0.8930\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 2.1350e-04 - acc: 1.0000 - val_loss: 0.5971 - val_acc: 0.8920\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 1.6347e-04 - acc: 1.0000 - val_loss: 0.5822 - val_acc: 0.8960\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 1.3985e-04 - acc: 1.0000 - val_loss: 0.5876 - val_acc: 0.8950\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 1.3317e-04 - acc: 1.0000 - val_loss: 0.5952 - val_acc: 0.8960\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 2.0203e-04 - acc: 1.0000 - val_loss: 0.5845 - val_acc: 0.8980\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 396us/sample - loss: 1.1217e-04 - acc: 1.0000 - val_loss: 0.5995 - val_acc: 0.8960\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 396us/sample - loss: 1.1528e-04 - acc: 1.0000 - val_loss: 0.6078 - val_acc: 0.8950\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 400us/sample - loss: 1.7328e-04 - acc: 1.0000 - val_loss: 0.6072 - val_acc: 0.8960\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 1.0295e-04 - acc: 1.0000 - val_loss: 0.6264 - val_acc: 0.8930\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 1.3468e-04 - acc: 1.0000 - val_loss: 0.6194 - val_acc: 0.8920\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 393us/sample - loss: 1.0504e-04 - acc: 1.0000 - val_loss: 0.6069 - val_acc: 0.8940\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 398us/sample - loss: 1.1446e-04 - acc: 1.0000 - val_loss: 0.6023 - val_acc: 0.8990\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 407us/sample - loss: 7.9907e-05 - acc: 1.0000 - val_loss: 0.6254 - val_acc: 0.8950\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 399us/sample - loss: 9.8319e-05 - acc: 1.0000 - val_loss: 0.6348 - val_acc: 0.8940\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 414us/sample - loss: 6.8245e-05 - acc: 1.0000 - val_loss: 0.6373 - val_acc: 0.8930\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 417us/sample - loss: 8.8194e-05 - acc: 1.0000 - val_loss: 0.6405 - val_acc: 0.8940\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 410us/sample - loss: 7.2060e-05 - acc: 1.0000 - val_loss: 0.6469 - val_acc: 0.8940\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 395us/sample - loss: 7.8260e-05 - acc: 1.0000 - val_loss: 0.6468 - val_acc: 0.8940\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 397us/sample - loss: 6.6189e-05 - acc: 1.0000 - val_loss: 0.6498 - val_acc: 0.8940\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 393us/sample - loss: 4.9213e-05 - acc: 1.0000 - val_loss: 0.6429 - val_acc: 0.8960\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 391us/sample - loss: 6.6133e-05 - acc: 1.0000 - val_loss: 0.6558 - val_acc: 0.8940\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 393us/sample - loss: 6.8215e-05 - acc: 1.0000 - val_loss: 0.6632 - val_acc: 0.8950\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 393us/sample - loss: 5.7558e-05 - acc: 1.0000 - val_loss: 0.6446 - val_acc: 0.8940\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 393us/sample - loss: 5.1481e-05 - acc: 1.0000 - val_loss: 0.6651 - val_acc: 0.8940\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 5.4063e-05 - acc: 1.0000 - val_loss: 0.6650 - val_acc: 0.8930\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 4.5380e-05 - acc: 1.0000 - val_loss: 0.6681 - val_acc: 0.8930\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 392us/sample - loss: 5.3161e-05 - acc: 1.0000 - val_loss: 0.6631 - val_acc: 0.8940\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 4.1588e-05 - acc: 1.0000 - val_loss: 0.6638 - val_acc: 0.8960\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 5.5406e-05 - acc: 1.0000 - val_loss: 0.6603 - val_acc: 0.8970\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 4.3256e-05 - acc: 1.0000 - val_loss: 0.6897 - val_acc: 0.8940\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 3.9990e-05 - acc: 1.0000 - val_loss: 0.6973 - val_acc: 0.8930\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 390us/sample - loss: 3.1846e-05 - acc: 1.0000 - val_loss: 0.6821 - val_acc: 0.8940\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 2.7090e-05 - acc: 1.0000 - val_loss: 0.6925 - val_acc: 0.8940\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 2.8291e-05 - acc: 1.0000 - val_loss: 0.7240 - val_acc: 0.8890\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 3.4156e-05 - acc: 1.0000 - val_loss: 0.6779 - val_acc: 0.8980\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 3.2027e-05 - acc: 1.0000 - val_loss: 0.7109 - val_acc: 0.8930\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 2.5741e-05 - acc: 1.0000 - val_loss: 0.6958 - val_acc: 0.8950\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 3.4613e-05 - acc: 1.0000 - val_loss: 0.7355 - val_acc: 0.8910\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 2.3186e-05 - acc: 1.0000 - val_loss: 0.6983 - val_acc: 0.8940\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 2.6155e-05 - acc: 1.0000 - val_loss: 0.7205 - val_acc: 0.8940\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 382us/sample - loss: 3.4506e-05 - acc: 1.0000 - val_loss: 0.7246 - val_acc: 0.8930\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 2.0564e-05 - acc: 1.0000 - val_loss: 0.7036 - val_acc: 0.8940\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 2.3681e-05 - acc: 1.0000 - val_loss: 0.7232 - val_acc: 0.8930\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 1.7316e-05 - acc: 1.0000 - val_loss: 0.7238 - val_acc: 0.8940\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 2.0086e-05 - acc: 1.0000 - val_loss: 0.7248 - val_acc: 0.8930\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 1.5848e-05 - acc: 1.0000 - val_loss: 0.7137 - val_acc: 0.8960\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 1.9748e-05 - acc: 1.0000 - val_loss: 0.7300 - val_acc: 0.8930\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 1.9188e-05 - acc: 1.0000 - val_loss: 0.7147 - val_acc: 0.8950\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 1.5306e-05 - acc: 1.0000 - val_loss: 0.7322 - val_acc: 0.8950\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 1.3028e-05 - acc: 1.0000 - val_loss: 0.7373 - val_acc: 0.8960\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 1.5770e-05 - acc: 1.0000 - val_loss: 0.7147 - val_acc: 0.8990\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 387us/sample - loss: 1.4949e-05 - acc: 1.0000 - val_loss: 0.7726 - val_acc: 0.8920\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 2.0694e-05 - acc: 1.0000 - val_loss: 0.7241 - val_acc: 0.8970\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 1.2527e-05 - acc: 1.0000 - val_loss: 0.7436 - val_acc: 0.8940\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 1.4534e-05 - acc: 1.0000 - val_loss: 0.7473 - val_acc: 0.8960\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 388us/sample - loss: 1.1722e-05 - acc: 1.0000 - val_loss: 0.7536 - val_acc: 0.8930\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 1.0725e-05 - acc: 1.0000 - val_loss: 0.7502 - val_acc: 0.8970\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 2.6392e-05 - acc: 1.0000 - val_loss: 0.7611 - val_acc: 0.8930\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 379us/sample - loss: 1.1572e-05 - acc: 1.0000 - val_loss: 0.7513 - val_acc: 0.8970\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 1.1466e-05 - acc: 1.0000 - val_loss: 0.7569 - val_acc: 0.8950\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 8.1765e-06 - acc: 1.0000 - val_loss: 0.7689 - val_acc: 0.8940\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 389us/sample - loss: 1.6486e-05 - acc: 1.0000 - val_loss: 0.7769 - val_acc: 0.8930\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 1.2489e-05 - acc: 1.0000 - val_loss: 0.7756 - val_acc: 0.8920\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 386us/sample - loss: 1.1100e-05 - acc: 1.0000 - val_loss: 0.7476 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 377us/sample - loss: 8.1616e-06 - acc: 1.0000 - val_loss: 0.7777 - val_acc: 0.8940\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 381us/sample - loss: 9.6397e-06 - acc: 1.0000 - val_loss: 0.7583 - val_acc: 0.8970\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 385us/sample - loss: 9.6481e-06 - acc: 1.0000 - val_loss: 0.7885 - val_acc: 0.8930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 6.7349e-06 - acc: 1.0000 - val_loss: 0.7820 - val_acc: 0.8940\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 380us/sample - loss: 1.1302e-05 - acc: 1.0000 - val_loss: 0.7806 - val_acc: 0.8960\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 7.7359e-06 - acc: 1.0000 - val_loss: 0.7723 - val_acc: 0.8970\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 383us/sample - loss: 7.2002e-06 - acc: 1.0000 - val_loss: 0.8148 - val_acc: 0.8930\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 384us/sample - loss: 5.9774e-06 - acc: 1.0000 - val_loss: 0.8038 - val_acc: 0.8930\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 377us/sample - loss: 6.6924e-06 - acc: 1.0000 - val_loss: 0.8123 - val_acc: 0.8920\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 378us/sample - loss: 6.0310e-06 - acc: 1.0000 - val_loss: 0.8206 - val_acc: 0.8910\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 382us/sample - loss: 8.8185e-06 - acc: 1.0000 - val_loss: 0.7821 - val_acc: 0.8960\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 400us/sample - loss: 6.5210e-06 - acc: 1.0000 - val_loss: 0.8037 - val_acc: 0.8930\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 417us/sample - loss: 7.4562e-06 - acc: 1.0000 - val_loss: 0.8199 - val_acc: 0.8920\n",
      "23.4 s ± 253 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos el modelo\n",
    "\n",
    "Para probar implementar este modelo en el Tensorflow Serving Container guardamos el modelo y los test features para la invocación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('test_features.npy', test_features) \n",
    "np.save('test_labels.npy', test_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2.PRACTICA_GUIADA_Transfer_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
