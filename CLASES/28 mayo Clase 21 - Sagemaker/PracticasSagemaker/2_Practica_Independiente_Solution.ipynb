{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica Independiente\n",
    "\n",
    "En esta práctica vamos a utilizar el modelo que guardamos en la carpeta model en la práctica anterior y vamos a implementarlo en el Tensorflow Serving Container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 1. Importemos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as mlt\n",
    "import boto3\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2. SageMaker requiere que construyamos un archivo tar.gz para pasárselo al tensorflow serving container. Recuerden que el container puede recibir varios modelos, por eso al guardar el modelo lo pusimos en una carpeta llamada /1 y ahora vamos a comprimirla en formato .tar.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/\n",
      "1/assets/\n",
      "1/saved_model.pb\n",
      "1/variables/\n",
      "1/variables/variables.index\n",
      "1/variables/variables.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tar -czvf model.tar.gz 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 3. Tenemos que subir el model.tar.gz a S3 y guardarnos la ubicación en un string. Pueden usar el SDK a través de sagemaker.Session().upload_data o investigar el comando bash aws s3 cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'dh-serving/model'\n",
    "\n",
    "model_data = sagemaker.Session().upload_data(path='model.tar.gz', key_prefix=s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-452432741922/dh-serving/model/model.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4. Utilizar el SDK para crear el tensorflow_serving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Model\" object doesn't create a SageMaker Model until a Transform Job or Endpoint is created.\n",
    "tensorflow_serving_model = Model(model_data=model_data,\n",
    "                                 role=role,\n",
    "                                 framework_version='2.0',\n",
    "                                 sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4. Utilizar el método deploy para desplegar el modelo sobre una infraestructura. Como tipo de instancia pueden elegir el ml.m5.large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = tensorflow_serving_model.deploy(initial_instance_count=1,\n",
    "                                            instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 5. Carguemos en memoria los features y labels de test en variables utilizando np.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.load('test_features.npy')\n",
    "test_labels = np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 8192)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 6: Crear el payload para el modelo. Para eso necesario armar un diccionario con la key instances y una lista de instancias. Los numpy arrays no son serializables. Para hacerlos serializables, es necesario utilizar el método tolist().\n",
    "\n",
    "Recurden armar un predict_batch con una de las instancias. El shape de la variable predict_batch debería ser (1, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch = test_features[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "          'instances': predict_batch.tolist()\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 7: Pueden utilizar el método predict() del predictor para obtener las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = predictor.predict(payload)['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99954152]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
