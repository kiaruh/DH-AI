{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.PRACTICA_GUIADA_Custom_Dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6F7EWvb8Ay4H","colab_type":"text"},"source":["# Inteligencia Artificial\n","# Clase 33 - Detección de Objetos con YOLO\n","\n","En esta notebook vamos a reentrenar YOLO v4 para personalizar los objetos que puede detectar. Trabajaremos con un dataset de imágenes de saxofones obtenido del [Open Image Dataset de Google](https://storage.googleapis.com/openimages/web/index.html), una inmensa base de datos *open source* con millones de imágenes anotadas de 600 categorías distintas. Veremos cómo usar la implementación original de Darknet para hacer *fine tuning* del modelo partiendo de los pesos preentrenados.\n","\n","<img src=\"https://miro.medium.com/max/1396/1*Co8xD0IWPaBiWr-Xfu38dw.jpeg\">\n","\n","El código de esta notebook se basa mayormente en los ejemplos de [The AI Guy](https://github.com/theAIGuysCode), mencionados por Alexey Bochkovskiy en su [publicación de divulgación en Medium](https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe)."]},{"cell_type":"markdown","metadata":{"id":"iXRc1h4KJNrD","colab_type":"text"},"source":["\n","## Setup\n","\n","Para poder utilizar la implementación de Darknet de YOLO v4, seguiremos los siguientes pasos:\n","\n","1. Clonar el [repositorio oficial](https://github.com/AlexeyAB/darknet).\n","2. Ajustar el Makefile para habilitar OpenCV y el uso de la GPU.\n","3. Construir Darknet.\n","4. Descargar los pesos del modelo preentrenado en el dataset [COCO](https://cocodataset.org/#explore) (acrónimo de *Common Objects in Context*)."]},{"cell_type":"code","metadata":{"id":"9XPn6o3MtHEW","colab_type":"code","colab":{}},"source":["# Clonamos el repositorio oficial\n","!git clone https://github.com/AlexeyAB/darknet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1P1MTWJMXFR","colab_type":"code","colab":{}},"source":["# Ajustamos el Makefile para habilitar OpenCV y el uso de la GPU\n","%cd darknet\n","!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n","!sed -i 's/GPU=0/GPU=1/' Makefile\n","!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n","!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71seczWnMpG2","colab_type":"code","colab":{}},"source":["# Construimos Darknet\n","!make"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrR5Fu3HNhQ_","colab_type":"code","colab":{}},"source":["# Descargamos los pesos del modelo preentrenado en COCO\n","!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_f7nnKbhRWk","colab_type":"code","colab":{}},"source":["import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Definimos una función utilitaria para mostrar las predicciones del modelo\n","def imShow(path):\n","  image = cv2.imread(path)\n","  height, width = image.shape[:2]\n","  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n","\n","  fig = plt.gcf()\n","  fig.set_size_inches(18, 10)\n","  plt.axis(\"off\")\n","  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rr1seJ1xhSP5","colab_type":"code","colab":{}},"source":["# Hacemos una prueba para verificar que el setup se hizo correctamete\n","!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/giraffe.jpg\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cUnlnUW_G-b5","colab_type":"text"},"source":["##  Entrenando YOLO v4 en un dataset *custom*\n","\n","En lugar de utilizar el modelo preentrenado para hacer inferencia sobre nuevas imágenes, ahora nos vamos a concentrar en reentrenarlo para que aprenda a detectar objetos diferentes a los del dataset COCO.\n","\n","Para poder desarrollar un detector personalizado con YOLO v4, vamos a necesitar:\n","\n","*   Un dataset *custom* debidamente etiquetado.\n","*   Un nuevo archivo .cfg, que define la configuración del modelo y el entrenamiento.\n","*   Archivos obj.data y obj.names, que les especifican al modelo cómo leer el dataset y qué clases contiene.\n","*   Un archivo train.txt que contiene las rutas a las imágenes de entrenamiento (opcionalmente, podemos incluir un test.txt también)."]},{"cell_type":"markdown","metadata":{"id":"uoIIcSCYorKY","colab_type":"text"},"source":["### Obtención y preparación del dataset\n","\n","En esta oportunidad, utilizaremos un dataset de imágenes de saxos descargado de [Open Imagen Dataset](https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=detection&c=%2Fm%2F06ncr).\n","\n","YOLO v4 requiere que las etiquetas de cada imagen se encuentren detalladas dentro de un archivo `.txt` ubicado en el mismo directorio y con el mismo nombre que la imagen. La estructura de las anotaciones tiene que ser la siguiente:\n","\n","`<object-class> <x_center> <y_center> <width> <height>`,\n","\n","donde:\n","\n","- `<object-class>` es un número que codifica una clase (enteros que van de 0 a -(n_clases-1)-;\n","- `<x_center>`, `<y_center>`, `<width>` y `<height>` definen la ubicación y el tamaño del *bounding box*. Son valores relativos al ancho y alto de la imagen -valores float en el rango (0.0 to 1.0]-.\n","\n","Para facilitar la obtención de las imágenes y la correcta preparación de las etiquetas correspondientes, utilizamos el [OIDv4_ToolKit](https://github.com/theAIGuysCode/OIDv4_ToolKit), una herramienta específicamente desarrollada para *fetchear* datasets de esta base de datos libre disponibilizada por Google.\n","\n","**Nota**: en [este video](https://www.youtube.com/watch?v=_4A9inxGqRM) encontrarán un tutorial paso a paso sobre el uso del OIDv4_ToolKit.\n"]},{"cell_type":"code","metadata":{"id":"a_u1xKNrpInl","colab_type":"code","colab":{}},"source":["# Montamos el Google Drive para acceder facilmente a los archivos que necesitamos\n","%cd ..\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWKPKakaphGs","colab_type":"code","colab":{}},"source":["# Creamos un symbolic link para poder llamar al path \"/content/gdrive/My\\ Drive/\" como /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RO0ejKSDToC6","colab_type":"code","colab":{}},"source":["# El directorio /mydrive/yolov4 contiene los archivos que necesitamos \n","!ls /mydrive/yolov4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nN9a8b5UPwL","colab_type":"code","colab":{}},"source":["# Copiamos los zips con la data al entorno de trabajo de Colab\n","!cp /mydrive/yolov4/data/obj.zip /content/\n","!cp /mydrive/yolov4/data/test.zip /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RW7zWDqkuXYu","colab_type":"code","colab":{}},"source":["# Extraemos los archivos en la carpeta /darknet/data\n","%cd darknet\n","!unzip ../obj.zip -d data/\n","!unzip ../test.zip -d data/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYLrV1oE-F9i","colab_type":"code","colab":{}},"source":["import os\n","print('Hay {} imágenes en el set de train y {} en el set de test.'.format(len(os.listdir('./data/obj'))//2,\n","                                                                          len(os.listdir('./data/test'))//2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qJ-cctLhVisl","colab_type":"text"},"source":["### Configuración de la arquitectura: archivo `.cfg`\n","\n","YOLO v4 requiere de un archivo con extensión `.cfg` que define la configuración de la arquitectura de la red y otras cuestiones importantes para el entrenamiento. Hasta el momento, estuvimos utilizando la configuración propia de `yolov4.cfg`, pero ahora es preciso hacer algunos cambios para adaptar al modelo a nuestro propio dataset.\n","\n","Para agilizar el entrenamiento de nuestro detector de saxos, en lugar de usar el modelo completo, entrenaremos una versión más pequeña, llamada Tiny YOLO. Podemos tomar el molde de esta configuración del archivo `yolov4-tiny-custom.cfg` y adaptarlo siguiendo algunos [lineamientos sugeridos en el repositorio oficial de Darknet](https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects):\n","\n","- `batch` = 64\n","- `subdivisions` = 16\n","- `max_batches` = classes*2000 (pero no menos de 6000)\n","- `steps` = 0.8\\*classes, 0.9\\*classes\n","- `width` = 416 (o cualquier múltiplo de 32)\n","- `height` = 416 (o cualquier múltiplo de 32)\n","- `filters` = (classes + 5) * 3 en las tres capas convolucionales antes de las *layers* de YOLO.\n","\n","Nuestro archivo `yolov4-tiny-obj.cfg` ya tiene las modificaciones pertinentes, por lo que simplemente lo depositamos en el directorio darknet/cfg:"]},{"cell_type":"code","metadata":{"id":"9IKiOpoXWx2O","colab_type":"code","colab":{}},"source":["# Copiamos nuestro .cfg a la carpeta con los archivos de configuración\n","!cp /mydrive/yolov4/yolov4-tiny-obj.cfg ./cfg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsyslJbR0V_V","colab_type":"text"},"source":["### Archivos `obj.data` y `obj.names`\n","\n","Estos archivos permiten indicarle al modelo cuántas clases hay en el dataset (y cuáles son), así como la ubicación de los archivos desde los que leerá la data y el path del directorio en el que irá guardando automáticamente los *checkpoints*.\n","\n","Mientras que `obj.names` es simplemente una lista de los nombres de cada clase (una por línea en el archivo y respetando el orden de la codificación propia de las etiquetas), la estructura de `obj.data` debe ser la siguiente:\n","\n","```\n","classes = # classes\n","train   = path/to/train.txt\n","valid   = path/to/test.txt\n","names   = path/to/obj.names\n","backup  = path/to/backup/\n","```"]},{"cell_type":"code","metadata":{"id":"F0Elm5aUXLGq","colab_type":"code","colab":{}},"source":["# Copiamos los archivos  obj.data y obj.names a la carpeta /darknet/data\n","!cp /mydrive/yolov4/data/obj.names ./data\n","!cp /mydrive/yolov4/data/obj.data  ./data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVZDNPgyYdn-","colab_type":"text"},"source":["### Archivos `train.txt` y `test.txt`\n","\n","Por último, sólo nos queda generar los archivos `train.txt` y `test.txt`, que contienen las rutas de las imágenes de entrenamiento y testeo (una por línea). Para generarlos, usaremos los *scripts* utilitarios `generate_train.py` y `generate_test.py`, cortesía de [The AI Guy](https://github.com/theAIGuysCode/YOLOv4-Cloud-Tutorial/tree/master/yolov4):"]},{"cell_type":"code","metadata":{"id":"40BrOy60Ugvn","colab_type":"code","colab":{}},"source":["# Copiamos los programas\n","!cp /mydrive/yolov4/generate_train.py ./\n","!cp /mydrive/yolov4/generate_test.py ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GYHg_PUUrmH","colab_type":"code","colab":{}},"source":["# Los ejecutamos...\n","!python generate_train.py\n","!python generate_test.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWPRvHnj3vmC","colab_type":"code","colab":{}},"source":["# ... y verificamos que se hayan generado los archivos train.txt y test.txt en darknet/data\n","!ls data/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X8WyTrEt4FJ1","colab_type":"text"},"source":["¡Excelente! Ya estamos en condiciones de entrenar nuestro propio detector personalizado. En lugar de entrenarlo desde cero, como es habitual, aprovecharemos las virtudes del *transfer learning* y tomaremos los pesos preentrenados de las primeras 29 capas convolucionales como punto de partida:"]},{"cell_type":"code","metadata":{"id":"DUXYBUmT5y8z","colab_type":"code","colab":{}},"source":["!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oxDCSlcB6F7H","colab_type":"text"},"source":["¡Perfecto! Ahora sí, sólo nos queda comenzar a entrenar. El comando para hacerlo será\n","\n","```\n","# En Linux\n","./darknet detector train <path to obj.data> <path to custom config> [<path to pre-trained weights>] [-dont_show] [-map]\n","\n","# En WindowsLinux\n","darknet.exe detector train <path to obj.data> <path to custom config> [<path to pre-trained weights>] [-dont_show] [-map]\n","```\n","\n","Aunque no es estrictamente necesario, el *flag* `-map` nos permite evaluar la métrica mean Average Precision durante el entrenamiento sobre el conjunto de datos de validación."]},{"cell_type":"code","metadata":{"id":"Mmg69H9N77We","colab_type":"code","colab":{}},"source":["# Entrenamos custom detector (descomentar %%capture si incurrimos en problemas de memoria o si Colab crashea)\n","#%%capture\n","!./darknet detector train data/obj.data cfg/yolov4-tiny-obj.cfg yolov4-tiny.conv.29 -dont_show -map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYkBNu5C8saj","colab_type":"text"},"source":["Como el entrenamiento es algo costoso, ya contamos con los pesos guardados cada 1000 iteraciones completas. Podemos comparar las métricas de mAP alcanzadas a lo largo del entrenamiento fácilmente:"]},{"cell_type":"code","metadata":{"id":"Et5QWA-d8-3a","colab_type":"code","colab":{}},"source":["# El comando map permite evaluar esta métrica\n","!./darknet detector map data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_1000.weights # Al cabo de 1000 iteraciones"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgLqvl8B9eUk","colab_type":"code","colab":{}},"source":["# El comando map permite evaluar esta métrica\n","!./darknet detector map data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_3000.weights # Al cabo de 3000 iteraciones"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iD1xjYjO9TWn","colab_type":"code","colab":{}},"source":["# El comando map permite evaluar esta métrica\n","!./darknet detector map data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_6000.weights # Al cabo de 6000 iteraciones"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ir1ywlVe89ga","colab_type":"text"},"source":["Vemos cómo nuestro modelo va mejorando hasta concluir su entrenamiento con un 84% de mAP. ¡Nada mal para un dataset de menos de 1000 imágenes!"]},{"cell_type":"markdown","metadata":{"id":"6Jfffxn5_R8F","colab_type":"text"},"source":["## Probando nuestro detector sobre imágenes nuevas\n","\n","Llego el momento de la verdad. Veamos cuán bien funciona nuestro detecto con imágenes nuevas:"]},{"cell_type":"code","metadata":{"id":"MzRgjbuQ_col","colab_type":"code","colab":{}},"source":["# Primero hacemos inferencia con el modelo preentrenado en COCO para comparar los resultados\n","!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights /mydrive/yolov4/imgs/bird.jpg\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSUK478d_wRe","colab_type":"code","colab":{}},"source":["# Ahora, corremos nuestro propio detector\n","!./darknet detector test data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_6000.weights /mydrive/yolov4/imgs/bird.jpg\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rbeHbHXAeFZ","colab_type":"text"},"source":["¡Fantástico! Veamos algunos ejemplos más..."]},{"cell_type":"code","metadata":{"id":"tnsedVJPAf5G","colab_type":"code","colab":{}},"source":["!./darknet detector test data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_6000.weights /mydrive/yolov4/imgs/coltrane.jpg\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2WHgoaX1BGNs","colab_type":"text"},"source":["¡Parece que nuestro detector es un éxito!\n","\n","Veamos ahora algunos ejemplos más difíciles de detectar correctamente:"]},{"cell_type":"code","metadata":{"id":"z9swZ3UrBI6w","colab_type":"code","colab":{}},"source":["!./darknet detector test data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_6000.weights /mydrive/yolov4/imgs/colley.jpg\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4D_Y_D5uAn3F","colab_type":"code","colab":{}},"source":["!./darknet detector test data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_6000.weights /mydrive/yolov4/imgs/bird_draw.jpg\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8yL1_JJABXL9","colab_type":"text"},"source":["En estos últimos casos, el modelo no parece estar prediciendo del todo bien... Veamos qué ocurre si reducimos el umbral de confianza:"]},{"cell_type":"code","metadata":{"id":"1x-f6_9rBj6Q","colab_type":"code","colab":{}},"source":["!./darknet detector test data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_6000.weights /mydrive/yolov4/imgs/colley.jpg -thresh 0.1\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ac3lbvLvBgH7","colab_type":"code","colab":{}},"source":["!./darknet detector test data/obj.data cfg/yolov4-tiny-obj.cfg /mydrive/yolov4/backup/yolov4-tiny-obj_6000.weights /mydrive/yolov4/imgs/bird_draw.jpg -thresh 0.1\n","imShow('predictions.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sy7vUk1YAkPe","colab_type":"text"},"source":["Sencillamente genial."]},{"cell_type":"markdown","metadata":{"id":"WvDBWh1U7JAd","colab_type":"text"},"source":["## Conclusines\n","\n","- Vimos cómo  utilizar podemos construir un detector de objetos personalizado reentrenando YOLO v4 sobre un dataset propio.\n","- Exploramos el Open Image Dataset, una gran fuente de imágenes ya etiquetadas y listas para alimentar algoritmos de detección.\n","- La herramienta OIDv4_ToolKit nos facilita la descarga y preparación del dataset.\n","- En caso de querer generar nuestras propias etiquetas, existen algunas aplicaciones que nos ayudan en la tarea, como [LabelImg](https://github.com/tzutalin/labelImg) o [OpenLabeling](https://github.com/Cartucho/OpenLabeling)."]}]}